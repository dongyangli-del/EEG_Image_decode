{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import clip\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "train = False\n",
    "classes = None\n",
    "pictures= None\n",
    "\n",
    "def load_data():\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    texts = []\n",
    "    images = []\n",
    "    \n",
    "    if train:\n",
    "        text_directory = \"/home/ldy/Workspace/THINGS/images_set/training_images\"  \n",
    "    else:\n",
    "        text_directory = \"/home/ldy/Workspace/THINGS/images_set/test_images\"\n",
    "    # 获取该路径下的所有目录\n",
    "    dirnames = [d for d in os.listdir(text_directory) if os.path.isdir(os.path.join(text_directory, d))]\n",
    "    dirnames.sort()\n",
    "    \n",
    "    if classes is not None:\n",
    "        dirnames = [dirnames[i] for i in classes]\n",
    "\n",
    "    for dir in dirnames:\n",
    "        # 尝试找到第一个'_'的位置\n",
    "        try:\n",
    "            idx = dir.index('_')\n",
    "            description = dir[idx+1:]  # 从第一个'_'之后取得所有内容\n",
    "        except ValueError:\n",
    "            print(f\"Skipped: {dir} due to no '_' found.\")\n",
    "            continue\n",
    "            \n",
    "        new_description = f\"{description}\"\n",
    "        texts.append(new_description)\n",
    "\n",
    "    if train:\n",
    "        img_directory = \"/home/ldy/Workspace/THINGS/images_set/training_images\"  # 请将其替换为你的新地址\n",
    "    else:\n",
    "        img_directory =\"/home/ldy/Workspace/THINGS/images_set/test_images\"\n",
    "    \n",
    "    all_folders = [d for d in os.listdir(img_directory) if os.path.isdir(os.path.join(img_directory, d))]\n",
    "    all_folders.sort()  # 保证文件夹的顺序\n",
    "\n",
    "    if classes is not None and pictures is not None:\n",
    "        images = []  # 初始化images列表\n",
    "        for i in range(len(classes)):\n",
    "            class_idx = classes[i]\n",
    "            pic_idx = pictures[i]\n",
    "            if class_idx < len(all_folders):\n",
    "                folder = all_folders[class_idx]\n",
    "                folder_path = os.path.join(img_directory, folder)\n",
    "                all_images = [img for img in os.listdir(folder_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                all_images.sort()\n",
    "                if pic_idx < len(all_images):\n",
    "                    images.append(os.path.join(folder_path, all_images[pic_idx]))\n",
    "    elif classes is not None and pictures is None:\n",
    "        images = []  # 初始化images列表\n",
    "        for i in range(len(classes)):\n",
    "            class_idx = classes[i]\n",
    "            if class_idx < len(all_folders):\n",
    "                folder = all_folders[class_idx]\n",
    "                folder_path = os.path.join(img_directory, folder)\n",
    "                all_images = [img for img in os.listdir(folder_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                all_images.sort()\n",
    "                images.extend(os.path.join(folder_path, img) for img in all_images)\n",
    "    elif classes is None:\n",
    "        images = []  # 初始化images列表\n",
    "        for folder in all_folders:\n",
    "            folder_path = os.path.join(img_directory, folder)\n",
    "            all_images = [img for img in os.listdir(folder_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            all_images.sort()  \n",
    "            images.extend(os.path.join(folder_path, img) for img in all_images)\n",
    "    else:\n",
    "        # 处理其他情况，比如 classes 和 pictures 长度不匹配\n",
    "        print(\"Error\")\n",
    "    return texts, images\n",
    "texts, images = load_data()\n",
    "# images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aircraft_carrier',\n",
       " 'antelope',\n",
       " 'backscratcher',\n",
       " 'balance_beam',\n",
       " 'banana',\n",
       " 'baseball_bat',\n",
       " 'basil',\n",
       " 'basketball',\n",
       " 'bassoon',\n",
       " 'baton4',\n",
       " 'batter',\n",
       " 'beaver',\n",
       " 'bench',\n",
       " 'bike',\n",
       " 'birthday_cake',\n",
       " 'blowtorch',\n",
       " 'boat',\n",
       " 'bok_choy',\n",
       " 'bonnet',\n",
       " 'bottle_opener',\n",
       " 'brace',\n",
       " 'bread',\n",
       " 'breadbox',\n",
       " 'bug',\n",
       " 'buggy',\n",
       " 'bullet',\n",
       " 'bun',\n",
       " 'bush',\n",
       " 'calamari',\n",
       " 'candlestick',\n",
       " 'cart',\n",
       " 'cashew',\n",
       " 'cat',\n",
       " 'caterpillar',\n",
       " 'cd_player',\n",
       " 'chain',\n",
       " 'chaps',\n",
       " 'cheese',\n",
       " 'cheetah',\n",
       " 'chest2',\n",
       " 'chime',\n",
       " 'chopsticks',\n",
       " 'cleat',\n",
       " 'cleaver',\n",
       " 'coat',\n",
       " 'cobra',\n",
       " 'coconut',\n",
       " 'coffee_bean',\n",
       " 'coffeemaker',\n",
       " 'cookie',\n",
       " 'cordon_bleu',\n",
       " 'coverall',\n",
       " 'crab',\n",
       " 'creme_brulee',\n",
       " 'crepe',\n",
       " 'crib',\n",
       " 'croissant',\n",
       " 'crow',\n",
       " 'cruise_ship',\n",
       " 'crumb',\n",
       " 'cupcake',\n",
       " 'dagger',\n",
       " 'dalmatian',\n",
       " 'dessert',\n",
       " 'dragonfly',\n",
       " 'dreidel',\n",
       " 'drum',\n",
       " 'duffel_bag',\n",
       " 'eagle',\n",
       " 'eel',\n",
       " 'egg',\n",
       " 'elephant',\n",
       " 'espresso',\n",
       " 'face_mask',\n",
       " 'ferry',\n",
       " 'flamingo',\n",
       " 'folder',\n",
       " 'fork',\n",
       " 'freezer',\n",
       " 'french_horn',\n",
       " 'fruit',\n",
       " 'garlic',\n",
       " 'glove',\n",
       " 'golf_cart',\n",
       " 'gondola',\n",
       " 'goose',\n",
       " 'gopher',\n",
       " 'gorilla',\n",
       " 'grasshopper',\n",
       " 'grenade',\n",
       " 'hamburger',\n",
       " 'hammer',\n",
       " 'handbrake',\n",
       " 'headscarf',\n",
       " 'highchair',\n",
       " 'hoodie',\n",
       " 'hummingbird',\n",
       " 'ice_cube',\n",
       " 'ice_pack',\n",
       " 'jeep',\n",
       " 'jelly_bean',\n",
       " 'jukebox',\n",
       " 'kettle',\n",
       " 'kneepad',\n",
       " 'ladle',\n",
       " 'lamb',\n",
       " 'lampshade',\n",
       " 'laundry_basket',\n",
       " 'lettuce',\n",
       " 'lightning_bug',\n",
       " 'manatee',\n",
       " 'marijuana',\n",
       " 'meatloaf',\n",
       " 'metal_detector',\n",
       " 'minivan',\n",
       " 'modem',\n",
       " 'mosquito',\n",
       " 'muff',\n",
       " 'music_box',\n",
       " 'mussel',\n",
       " 'nightstand',\n",
       " 'okra',\n",
       " 'omelet',\n",
       " 'onion',\n",
       " 'orange',\n",
       " 'orchid',\n",
       " 'ostrich',\n",
       " 'pajamas',\n",
       " 'panther',\n",
       " 'paperweight',\n",
       " 'pear',\n",
       " 'pepper1',\n",
       " 'pheasant',\n",
       " 'pickax',\n",
       " 'pie',\n",
       " 'pigeon',\n",
       " 'piglet',\n",
       " 'pocket',\n",
       " 'pocketknife',\n",
       " 'popcorn',\n",
       " 'popsicle',\n",
       " 'possum',\n",
       " 'pretzel',\n",
       " 'pug',\n",
       " 'punch2',\n",
       " 'purse',\n",
       " 'radish',\n",
       " 'raspberry',\n",
       " 'recorder',\n",
       " 'rhinoceros',\n",
       " 'robot',\n",
       " 'rooster',\n",
       " 'rug',\n",
       " 'sailboat',\n",
       " 'sandal',\n",
       " 'sandpaper',\n",
       " 'sausage',\n",
       " 'scallion',\n",
       " 'scallop',\n",
       " 'scooter',\n",
       " 'seagull',\n",
       " 'seaweed',\n",
       " 'seed',\n",
       " 'skateboard',\n",
       " 'sled',\n",
       " 'sleeping_bag',\n",
       " 'slide',\n",
       " 'slingshot',\n",
       " 'snowshoe',\n",
       " 'spatula',\n",
       " 'spoon',\n",
       " 'station_wagon',\n",
       " 'stethoscope',\n",
       " 'strawberry',\n",
       " 'submarine',\n",
       " 'suit',\n",
       " 't-shirt',\n",
       " 'table',\n",
       " 'taillight',\n",
       " 'tape_recorder',\n",
       " 'television',\n",
       " 'tiara',\n",
       " 'tick',\n",
       " 'tomato_sauce',\n",
       " 'tongs',\n",
       " 'tool',\n",
       " 'top_hat',\n",
       " 'treadmill',\n",
       " 'tube_top',\n",
       " 'turkey',\n",
       " 'unicycle',\n",
       " 'vise',\n",
       " 'volleyball',\n",
       " 'wallpaper',\n",
       " 'walnut',\n",
       " 'wheat',\n",
       " 'wheelchair',\n",
       " 'windshield',\n",
       " 'wine',\n",
       " 'wok']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'image_generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 176\u001b[0m\n\u001b[1;32m    172\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_eeg(eeg_embedding)\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out  \n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimage_generate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_eegfeatures\u001b[39m(sub, eegmodel, dataloader, device, text_features_all, img_features_all, k, mode):\n\u001b[1;32m    178\u001b[0m     eegmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'image_generate'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "# os.environ['http_proxy'] = 'http://10.16.35.10:13390' \n",
    "# os.environ['https_proxy'] = 'http://10.16.35.10:13390' \n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"KEY\"\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "from itertools import combinations\n",
    "\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "from eegdatasets_leaveone import EEGDataset\n",
    "from eegencoder import eeg_encoder\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from lavis.models.clip_models.loss import ClipLoss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from utils import wandb_logger\n",
    "from torch import Tensor\n",
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # 计算额外的一个元素以适应奇数维度\n",
    "        div_term = torch.exp(torch.arange(0, d_model + 1, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # 使用切片确保不会溢出\n",
    "        pe[:, 0::2] = torch.sin(position * div_term[:d_model // 2 + 1])\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[:d_model // 2])\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pe = self.pe[:x.size(0), :].unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        x = x + pe\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EEGAttention(nn.Module):\n",
    "    def __init__(self, channel, d_model, nhead):\n",
    "        super(EEGAttention, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        self.channel = channel\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.permute(2, 0, 1)  # Change shape to [time_length, batch_size, channel]\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output.permute(1, 2, 0)  # Change shape back to [batch_size, channel, time_length]\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=40):\n",
    "        super().__init__()\n",
    "        # revised from shallownet\n",
    "        self.tsconv = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 5), (1, 1)),\n",
    "            nn.AvgPool2d((1, 17), (1, 5)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(40, 40, (63, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  \n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # b, _, _, _ = x.shape\n",
    "        x = x.unsqueeze(1)     \n",
    "        # print(\"x\", x.shape)   \n",
    "        x = self.tsconv(x)\n",
    "        # print(\"tsconv\", x.shape)   \n",
    "        x = self.projection(x)\n",
    "        # print(\"projection\", x.shape)  \n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FlattenHead(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Enc_eeg(nn.Sequential):\n",
    "    def __init__(self, emb_size=40, **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(emb_size),\n",
    "            FlattenHead()\n",
    "        )\n",
    "\n",
    "        \n",
    "class Proj_eeg(nn.Sequential):\n",
    "    def __init__(self, embedding_dim=1840, proj_dim=1024, drop_proj=0.5):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_dim, proj_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(proj_dim, proj_dim),\n",
    "                nn.Dropout(drop_proj),\n",
    "            )),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "class Proj_img(nn.Sequential):\n",
    "    def __init__(self, embedding_dim=1024, proj_dim=1024, drop_proj=0.3):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_dim, proj_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(proj_dim, proj_dim),\n",
    "                nn.Dropout(drop_proj),\n",
    "            )),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x \n",
    "\n",
    "class ATM_S_reconstruction_scale_0_1000(nn.Module):    \n",
    "    def __init__(self, num_channels=63, sequence_length=25, num_subjects=1, num_features=64, num_latents=1024, num_blocks=1):\n",
    "        super(ATM_S_reconstruction_scale_0_1000, self).__init__()\n",
    "        self.attention_model = EEGAttention(num_channels, num_channels, nhead=1)   \n",
    "        self.subject_wise_linear = nn.ModuleList([nn.Linear(sequence_length, sequence_length) for _ in range(num_subjects)])\n",
    "        self.enc_eeg = Enc_eeg()\n",
    "        self.proj_eeg = Proj_eeg()        \n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.loss_func = ClipLoss()       \n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = self.attention_model(x)\n",
    "        # print(f'After attention shape: {x.shape}')\n",
    "         \n",
    "        x = self.subject_wise_linear[0](x)\n",
    "        # print(f'After subject-specific linear transformation shape: {x.shape}')\n",
    "        eeg_embedding = self.enc_eeg(x)\n",
    "        # print(f'After enc_eeg shape: {eeg_embedding.shape}')\n",
    "        out = self.proj_eeg(eeg_embedding)\n",
    "        return out  \n",
    "    \n",
    "    \n",
    "from image_generate import *\n",
    "def get_eegfeatures(sub, eegmodel, dataloader, device, text_features_all, img_features_all, k, mode):\n",
    "    eegmodel.eval()\n",
    "    text_features_all = text_features_all.to(device).float()\n",
    "    img_features_all = img_features_all.to(device).float()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    alpha =0.9\n",
    "    top5_correct = 0\n",
    "    top5_correct_count = 0\n",
    "    # 获取所有独特的类别\n",
    "    all_labels = set(range(text_features_all.size(0)))\n",
    "    top5_acc = 0\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    ridge_lambda = 0.1\n",
    "    save_features = True\n",
    "    features_list = []  # List to store features    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (eeg_data, labels, text, text_features, img, img_features) in enumerate(dataloader):\n",
    "            eeg_data = eeg_data.to(device)\n",
    "            eeg_data = eeg_data[:, :, :250]\n",
    "            # print(\"eeg_data\", eeg_data.shape)\n",
    "            text_features = text_features.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            img_features = img_features.to(device).float()\n",
    "            eeg_features = eegmodel(eeg_data).float()\n",
    "            features_list.append(eeg_features)\n",
    "            logit_scale = eegmodel.logit_scale \n",
    "                   \n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)\n",
    "            # print(\"eeg_features\", eeg_features.shape)\n",
    "            # print(torch.std(eeg_features, dim=-1))\n",
    "            # print(torch.std(img_features, dim=-1))\n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            # loss = (regress_loss + ridge_lambda * l2_norm)       \n",
    "            img_loss = eegmodel.loss_func(eeg_features, img_features, logit_scale)\n",
    "            text_loss = eegmodel.loss_func(eeg_features, text_features, logit_scale)\n",
    "            contrastive_loss = img_loss\n",
    "            # loss = img_loss + text_loss\n",
    "\n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)\n",
    "            # print(\"text_loss\", text_loss)\n",
    "            # print(\"img_loss\", img_loss)\n",
    "            # print(\"regress_loss\", regress_loss)            \n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            # loss = (regress_loss + ridge_lambda * l2_norm)       \n",
    "            loss = alpha * regress_loss *10 + (1 - alpha) * contrastive_loss*10\n",
    "            # print(\"loss\", loss)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            for idx, label in enumerate(labels):\n",
    "                # 先从除了正确类别之外的类别中选择 k-1 个\n",
    "                possible_classes = list(all_labels - {label.item()})\n",
    "                selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                selected_img_features = img_features_all[selected_classes]\n",
    "                \n",
    "                # 计算对应的 logits\n",
    "                logits_img = logit_scale * eeg_features[idx] @ selected_img_features.T\n",
    "                # logits_text = logit_scale * eeg_features[idx] @ selected_text_features.T\n",
    "                # logits_single = (logits_text + logits_img) / 2.0\n",
    "                logits_single = logits_img\n",
    "                # print(\"logits_single\", logits_single.shape)\n",
    "                # 获取预测的类别\n",
    "                # predicted_label = selected_classes[torch.argmax(logits_single).item()]\n",
    "                predicted_label = selected_classes[torch.argmax(logits_single).item()] # (n_batch, ) \\in {0, 1, ..., n_cls-1}\n",
    "                if predicted_label == label.item():\n",
    "                    correct += 1        \n",
    "                total += 1\n",
    "\n",
    "        if save_features:\n",
    "            features_tensor = torch.cat(features_list, dim=0)\n",
    "            print(\"features_tensor\", features_tensor.shape)\n",
    "            torch.save(features_tensor.cpu(), f\"ATM_S_eeg_features_{sub}_{mode}.pt\")  # Save features as .pt file\n",
    "    average_loss = total_loss / (batch_idx+1)\n",
    "    accuracy = correct / total\n",
    "    return average_loss, accuracy, labels, features_tensor.cpu()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "config = {\n",
    "\"data_path\": \"/home/ldy/Workspace/THINGS/Preprocessed_data_250Hz\",\n",
    "\"project\": \"train_pos_img_text_rep\",\n",
    "\"entity\": \"sustech_rethinkingbci\",\n",
    "\"name\": \"lr=3e-4_img_pos_pro_eeg\",\n",
    "\"lr\": 3e-4,\n",
    "\"epochs\": 50,\n",
    "\"batch_size\": 1024,\n",
    "\"logger\": True,\n",
    "\"encoder_type\":'ATM_S_reconstruction_scale_0_1000',\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 获取数据路径\n",
    "data_path = config['data_path']\n",
    "emb_img_test = torch.load('variables/ViT-H-14_features_test.pt')\n",
    "emb_img_train = torch.load('variables/ViT-H-14_features_train.pt')\n",
    "# 模型和优化器定义\n",
    "eeg_model = ATM_S_reconstruction_scale_0_1000(63, 250)\n",
    "print('number of parameters:', sum([p.numel() for p in eeg_model.parameters()]))\n",
    "\n",
    "#####################################################################################\n",
    "# 确定使用的设备\n",
    "# eeg_model.load_state_dict(torch.load(\"/home/ldy/Workspace/Reconstruction/models/contrast/sub-08/01-30_00-44/40.pth\"))\n",
    "eeg_model.load_state_dict(torch.load(\"models/contrast/ATM_S_reconstruction_scale_0_1000/02-01_00-05/sub-05/40.pth\"))\n",
    "eeg_model = eeg_model.to(device)\n",
    "sub = 'sub-05'\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "test_dataset = EEGDataset(data_path, subjects= [sub], train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "text_features_test_all = test_dataset.text_features\n",
    "img_features_test_all = test_dataset.img_features\n",
    "test_loss, test_accuracy,labels, eeg_features_test = get_eegfeatures(sub, eeg_model, test_loader, device, text_features_test_all, img_features_test_all,k=200, mode=\"test\")\n",
    "print(f\" - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "train_dataset = EEGDataset(data_path, subjects= [sub], train=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "text_features_test_all = train_dataset.text_features\n",
    "img_features_test_all = train_dataset.img_features\n",
    "\n",
    "train_loss, train_accuracy, labels, eeg_features_train = get_eegfeatures(sub, eeg_model, train_loader, device, text_features_test_all, img_features_test_all,k=200, mode=\"train\")\n",
    "print(f\" - Test Loss: {train_loss:.4f}, Test Accuracy: {train_accuracy:.4f}\")\n",
    "#####################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_img_train_4 = emb_img_train.view(1654,10,1,1024).repeat(1,1,4,1).view(-1,1024)\n",
    "emb_eeg = torch.load(f'/home/ldy/Workspace/Reconstruction/ATM_S_eeg_features_{sub}_train.pt')\n",
    "emb_eeg_test = torch.load(f'/home/ldy/Workspace/Reconstruction/ATM_S_eeg_features_{sub}_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_eeg.shape, emb_eeg_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EmbeddingDataset(\n",
    "    c_embeddings=eeg_features_train, h_embeddings=emb_img_train_4, \n",
    "    # h_embeds_uncond=h_embeds_imgnet\n",
    ")\n",
    "dl = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=64)\n",
    "diffusion_prior = DiffusionPriorUNet(cond_dim=1024, dropout=0.1)\n",
    "# number of parameters\n",
    "print(sum(p.numel() for p in diffusion_prior.parameters() if p.requires_grad))\n",
    "pipe = Pipe(diffusion_prior, device=device)\n",
    "\n",
    "# load pretrained model\n",
    "model_name = 'diffusion_prior' # 'diffusion_prior_vice_pre_imagenet' or 'diffusion_prior_vice_pre'\n",
    "# pipe.train(dl, num_epochs=150, learning_rate=1e-3) # to 0.142 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe.diffusion_prior.load_state_dict(torch.load(f'./fintune_ckpts/{config['data_path']}/{sub}/{model_name}.pt', map_location=device))\n",
    "# save_path = f'./fintune_ckpts/{config[\"encoder_type\"]}/{sub}/{model_name}.pt'\n",
    "\n",
    "# directory = os.path.dirname(save_path)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "# os.makedirs(directory, exist_ok=True)\n",
    "# torch.save(pipe.diffusion_prior.state_dict(), save_path)\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Assuming generator.generate returns a PIL Image\n",
    "generator = Generator4Embeds(num_inference_steps=4, device=device)\n",
    "\n",
    "directory = f\"generated_imgs/{sub}\"\n",
    "for k in range(200):\n",
    "    eeg_embeds = emb_eeg_test[k:k+1]\n",
    "    h = pipe.generate(c_embeds=eeg_embeds, num_inference_steps=50, guidance_scale=5.0)\n",
    "    for j in range(1):\n",
    "        image = generator.generate(h.to(dtype=torch.float16))\n",
    "        # Construct the save path for each image\n",
    "        path = f'{directory}/{texts[k]}/{j}.png'\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        # Save the PIL Image\n",
    "        image.save(path)\n",
    "        # print(f'Image saved to {path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# # os.environ['http_proxy'] = 'http://10.16.35.10:13390' \n",
    "# # os.environ['https_proxy'] = 'http://10.16.35.10:13390' \n",
    "# # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\" \n",
    "# # os.environ['PATH'] += os.pathsep + '/usr/local/texlive/2023/bin/x86_64-linux'\n",
    "\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "# import torchvision.transforms as transforms\n",
    "# import matplotlib.pyplot as plt\n",
    "# import open_clip\n",
    "# from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# import sys\n",
    "# from diffusion_prior import *\n",
    "# from custom_pipeline import *\n",
    "\n",
    "# device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load eeg and image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load image embeddings\n",
    "# data = torch.load('/home/ldy/Workspace/THINGS/CLIP/ViT-H-14_features_test.pt', map_location='cuda:3')\n",
    "# emb_img_test = data['img_features']\n",
    "\n",
    "# # load image embeddings\n",
    "# data = torch.load('/home/ldy/Workspace/THINGS/CLIP/ViT-H-14_features_train.pt', map_location='cuda:3')\n",
    "# emb_img_train = data['img_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_img_test = torch.load('variables/ViT-H-14_features_test.pt')\n",
    "# emb_img_train = torch.load('variables/ViT-H-14_features_train.pt')\n",
    "\n",
    "# torch.save(emb_img_test.cpu().detach(), 'variables/ViT-H-14_features_test.pt')\n",
    "# torch.save(emb_img_train.cpu().detach(), 'variables/ViT-H-14_features_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_img_test.shape, emb_img_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1654clsx10imgsx4trials=66160\n",
    "# emb_eeg = torch.load('/home/ldy/Workspace/Reconstruction/ATM_S_eeg_features_sub-08.pt')\n",
    "\n",
    "# emb_eeg_test = torch.load('/home/ldy/Workspace/Reconstruction/ATM_S_eeg_features_sub-08_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_eeg.shape, emb_eeg_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training prior diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "\n",
    "    def __init__(self, c_embeddings=None, h_embeddings=None, h_embeds_uncond=None, cond_sampling_rate=0.5):\n",
    "        self.c_embeddings = c_embeddings\n",
    "        self.h_embeddings = h_embeddings\n",
    "        self.N_cond = 0 if self.h_embeddings is None else len(self.h_embeddings)\n",
    "        self.h_embeds_uncond = h_embeds_uncond\n",
    "        self.N_uncond = 0 if self.h_embeds_uncond is None else len(self.h_embeds_uncond)\n",
    "        self.cond_sampling_rate = cond_sampling_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N_cond\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"c_embedding\": self.c_embeddings[idx],\n",
    "            \"h_embedding\": self.h_embeddings[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_img_train_4 = emb_img_train.view(1654,10,1,1024).repeat(1,1,4,1).view(-1,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_img_train_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_data = '/mnt/dataset0/weichen/projects/visobj/proposals/mise/data'\n",
    "# image_features = torch.load(os.path.join(path_data, 'openclip_emb/emb_imgnet.pt')) # 'emb_imgnet' or 'image_features'\n",
    "# h_embeds_imgnet = image_features['image_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset = EmbeddingDataset(\n",
    "    c_embeddings=emb_eeg, h_embeddings=emb_img_train_4, \n",
    "    # h_embeds_uncond=h_embeds_imgnet\n",
    ")\n",
    "print(len(dataset))\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion_prior = DiffusionPrior(dropout=0.1)\n",
    "diffusion_prior = DiffusionPriorUNet(cond_dim=1024, dropout=0.1)\n",
    "# number of parameters\n",
    "print(sum(p.numel() for p in diffusion_prior.parameters() if p.requires_grad))\n",
    "pipe = Pipe(diffusion_prior, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "model_name = 'diffusion_prior' # 'diffusion_prior_vice_pre_imagenet' or 'diffusion_prior_vice_pre'\n",
    "pipe.diffusion_prior.load_state_dict(torch.load(f'./ckpts/{model_name}.pt', map_location=device))\n",
    "# pipe.train(dataloader, num_epochs=150, learning_rate=1e-3) # to 0.142 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# torch.save(pipe.diffusion_prior.state_dict(), f'./ckpts/{model_name}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating by eeg embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# torch.save(pipe.diffusion_prior.state_dict(), f'./ckpts/{model_name}.pt')\n",
    "from IPython.display import Image, display\n",
    "generator = Generator4Embeds(num_inference_steps=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of ground truth: /home/ldy/Workspace/THINGS/images_set/test_images\n",
    "k = 99\n",
    "image_embeds = emb_img_test[k:k+1]\n",
    "print(\"image_embeds\", image_embeds.shape)\n",
    "image = generator.generate(image_embeds)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_embeds = emb_eeg_test[k:k+1]\n",
    "# print(\"image_embeds\", image_embeds.shape)\n",
    "# image = generator.generate(image_embeds)\n",
    "# display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating by eeg informed image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 0\n",
    "eeg_embeds = emb_eeg_test[k:k+1]\n",
    "print(\"image_embeds\", eeg_embeds.shape)\n",
    "h = pipe.generate(c_embeds=eeg_embeds, num_inference_steps=50, guidance_scale=5.0)\n",
    "image = generator.generate(h.to(dtype=torch.float16))\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
